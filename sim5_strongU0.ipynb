{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-27T21:49:16.360860Z",
     "start_time": "2024-11-27T21:49:15.622710Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from scipy.stats import pearsonr\n",
    "from IPython.display import display, Math, Markdown\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0. Introduction\n",
   "id": "1bceec036e2735e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Simulations in this document are for the approach outlined in draft 3.2. We generate data under settings such that sequential conditional exchangeability assumption holds with full set of covariates. We compute the population quantity of mean potential outcome under treatment path $\\bar{A} = \\bar{1}$ (1), the population AIPW $\\psi$ based on (8), the incorrect population AIPW $\\psi^*$ based on (11), their difference $\\psi^*-\\psi$ as listed (12). Moreover, we compute the basic ovb based on equation (13), updated ovb based on equation (26),  the ovb bound based on (14) together with (18) (19) (20), and the ovb bound based on (14), (18) (19) (20), and corollary 1.\n",
    "\n",
    "## 1. Data Generating \n",
    "\n",
    "### 1.1 Description\n",
    "\n",
    "\\begin{align*}\n",
    "U_0 &\\sim \\mathcal{N}(0, \\sigma_{U_0}^2), \\\\[1em]\n",
    "L_0 &= \\beta_{L_0|U_0} U_0 + \\epsilon_{L_0}, \\quad \\epsilon_{L_0} \\sim \\mathcal{N}(0, \\sigma_{L_0}^2), \\\\[1em]\n",
    "P(A_0 = 1 \\mid L_0 = \\ell_0, U_0 = u_0) &= \\left[1 + \\exp\\{- (\\beta_{A_0|L_0} \\ell_0 + \\beta_{A_0|U_0} u_0)\\}\\right]^{-1}, \\\\[1em]\n",
    "L_1 &= \\beta_{L_1|L_0} L_0 + \\beta_{L_1|A_0} A_0 + \\beta_{L_1|U_0} U_0 + \\epsilon_{L_1}, \\quad \\epsilon_{L_1} \\sim \\mathcal{N}(0, \\sigma_{L_1}^2), \\\\[1em]\n",
    "P(A_1 = 1 \\mid L_0 = \\ell_0, L_1 = \\ell_1, A_0 = a_0, U_0 = u_0) &= \\left[1 + \\exp\\{- (\\beta_{A_1|L_1} \\ell_1 + \\beta_{A_1|A_0} a_0 + \\beta_{A_1|L_0} \\ell_0 + \\beta_{A_1|U_0} u_0)\\}\\right]^{-1}, \\\\[1em]\n",
    "Y &= \\gamma_{Y|L_0} L_0 + \\gamma_{Y|L_1} L_1 + \\gamma_{Y|A_0} A_0 + \\gamma_{Y|A_1} A_1 + \\gamma_{Y|U_0} U_0 + \\epsilon_Y, \\quad \\epsilon_Y \\sim \\mathcal{N}(0, \\sigma_Y^2).\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta_{L0} &= (\\beta_{L_0|U_0} ) = 0.5, \\\\\n",
    "\\beta_{A_0} &= (\\beta_{A_0|L_0}, \\beta_{A_0|U_0}) = (0.5, 1.5),\\\\\n",
    "\\beta_{L_1} &= (\\beta_{L_1|L_0}, \\beta_{L_1|A_0}, \\beta_{L_1|U_0}) = (0.6, 0.4, 0.8), \\\\\n",
    "\\beta_{A_1} &= (\\beta_{A_1|L_1}, \\beta_{A_1|A_0}, \\beta_{A_1|L_0}, \\beta_{A_1|U_0}) = (0.4, 0.2, 0.3, 1.0), \\\\\n",
    "\\gamma_{Y} &= (\\gamma_{Y|L_0}, \\gamma_{Y|L_1} L_1, \\gamma_{Y|A_0} A_0,  \\gamma_{Y|A_1} , \\gamma_{Y|U_0}) = (1.2, 0.8, 0.5, 0.7, 2.0),\\\\\n",
    "\\sigma_{U_0} &= 1,\\\\\n",
    "\\sigma_{L_0} &= 1,\\\\\n",
    "\\sigma_{L_1} &= 1,\\\\\n",
    "\\sigma_Y &= 1.\n",
    "\\end{align*}"
   ],
   "id": "a68ce594ac9e4890"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:49:23.997778Z",
     "start_time": "2024-11-27T21:49:19.914395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setting seed\n",
    "np.random.seed(43)\n",
    "\n",
    "# Large sample size\n",
    "n = 20000000\n",
    "\n",
    "# Define parameters for the data-generating process\n",
    "params = {\n",
    "    \"U0_std\": 1.0,          # Standard deviation for U_0\n",
    "    \"L0_U0_coeff\": 0.5,     # Coefficient for U_0 in L_0\n",
    "    \"L0_noise_std\": 1.0,    # Standard deviation for noise in L_0\n",
    "    \"A0_L0_coeff\": 0.5,     # Coefficient for L_0 in A_0\n",
    "    \"A0_U0_coeff\": 1.5,     # Coefficient for U_0 in A_0\n",
    "    \"L1_L0_coeff\": 0.6,     # Coefficient for L_0 in L_1\n",
    "    \"L1_A0_coeff\": 0.4,     # Coefficient for A_0 in L_1\n",
    "    \"L1_U0_coeff\": 0.8,     # Coefficient for U_0 in L_1\n",
    "    \"L1_noise_std\": 1.0,    # Standard deviation for noise in L_1\n",
    "    \"A1_L1_coeff\": 0.4,     # Coefficient for L_1 in A_1\n",
    "    \"A1_A0_coeff\": 0.2,     # Coefficient for A_0 in A_1\n",
    "    \"A1_L0_coeff\": 0.3,     # Coefficient for L_0 in A_1\n",
    "    \"A1_U0_coeff\": 1.0,     # Coefficient for U_0 in A_1\n",
    "    \"Y_L0_coeff\": 1.2,      # Coefficient for L_0 in Y\n",
    "    \"Y_L1_coeff\": 0.8,      # Coefficient for L_1 in Y\n",
    "    \"Y_A0_coeff\": 0.5,      # Coefficient for A_0 in Y\n",
    "    \"Y_A1_coeff\": 0.7,      # Coefficient for A_1 in Y\n",
    "    \"Y_U0_coeff\": 2.0,      # Coefficient for U_0 in Y\n",
    "    \"Y_noise_std\": 1.0      # Standard deviation for noise in Y\n",
    "}\n",
    "\n",
    "# Step 1: Generate U0 (baseline unobserved covariates)\n",
    "U_0 = np.random.normal(0, params[\"U0_std\"], n)  # U_0 ~ Normal(0, std)\n",
    "\n",
    "# Step 2: Generate L0 (baseline observed covariates) as a function of U0\n",
    "L_0 = params[\"L0_U0_coeff\"] * U_0 + np.random.normal(0, params[\"L0_noise_std\"], n)\n",
    "\n",
    "# Step 3: Generate A0 (treatment at time 0) based on L0 and U0\n",
    "p_A0 = 1 / (1 + np.exp(-(params[\"A0_L0_coeff\"] * L_0 + params[\"A0_U0_coeff\"] * U_0)))\n",
    "A_0 = np.random.binomial(1, p_A0, n)\n",
    "\n",
    "# Step 4: Generate L1 (covariates at time 1) based on L0, U0, and A0\n",
    "L_1 = (params[\"L1_L0_coeff\"] * L_0 +\n",
    "       params[\"L1_A0_coeff\"] * A_0 +\n",
    "       params[\"L1_U0_coeff\"] * U_0 +\n",
    "       np.random.normal(0, params[\"L1_noise_std\"], n))\n",
    "\n",
    "# Step 5: Generate A1 (treatment at time 1) based on L0, U0, L1, and A0\n",
    "p_A1 = 1 / (1 + np.exp(-(params[\"A1_L1_coeff\"] * L_1 +\n",
    "                        params[\"A1_A0_coeff\"] * A_0 +\n",
    "                        params[\"A1_L0_coeff\"] * L_0 +\n",
    "                        params[\"A1_U0_coeff\"] * U_0)))\n",
    "A_1 = np.random.binomial(1, p_A1, n)\n",
    "\n",
    "# Step 6: Generate observed outcome Y based on L0, U0, A0, A1, and L1\n",
    "Y = (params[\"Y_L0_coeff\"] * L_0 +\n",
    "     params[\"Y_L1_coeff\"] * L_1 +\n",
    "     params[\"Y_A0_coeff\"] * A_0 +\n",
    "     params[\"Y_A1_coeff\"] * A_1 +\n",
    "     params[\"Y_U0_coeff\"] * U_0 +\n",
    "     np.random.normal(0, params[\"Y_noise_std\"], n))\n"
   ],
   "id": "447cba8af1875011",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:49:26.333524Z",
     "start_time": "2024-11-27T21:49:25.212719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 7: Regenerate L1 under the intervention A0=1\n",
    "# L1_bar1: covariates at time 1 assuming A0=1\n",
    "L1_bar1 = (params[\"L1_L0_coeff\"] * L_0 +\n",
    "           params[\"L1_A0_coeff\"] * 1 +  # Intervention A0=1\n",
    "           params[\"L1_U0_coeff\"] * U_0 +\n",
    "           np.random.normal(0, params[\"L1_noise_std\"], n))\n",
    "\n",
    "# Y_bar1: potential outcome under treatment path (A0=1, A1=1)\n",
    "Y_bar1 = (params[\"Y_L0_coeff\"] * L_0 +\n",
    "          params[\"Y_L1_coeff\"] * L1_bar1 +\n",
    "          params[\"Y_A0_coeff\"] * 1 +  # Intervention A0=1\n",
    "          params[\"Y_A1_coeff\"] * 1 +  # Intervention A1=1\n",
    "          params[\"Y_U0_coeff\"] * U_0 +\n",
    "          np.random.normal(0, params[\"Y_noise_std\"], n))"
   ],
   "id": "71d2c770e7558b8f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:49:30.373679Z",
     "start_time": "2024-11-27T21:49:27.860776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Combine data into a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    \"L_0\": L_0,\n",
    "    \"U_0\": U_0,\n",
    "    \"A_0\": A_0,\n",
    "    \"L_1\": L_1,\n",
    "    \"A_1\": A_1,\n",
    "    \"Y\": Y,\n",
    "    \"L1_bar1\": L1_bar1,\n",
    "    \"Y_bar1\": Y_bar1\n",
    "})\n",
    "\n",
    "print(data.shape)"
   ],
   "id": "eeadede230a51af0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000000, 8)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 Check sequential conditional exchangeability holds with full set of covariates via ROC AUC\n",
   "id": "5c5c8ea8235e4cfb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:49:31.874788Z",
     "start_time": "2024-11-27T21:49:31.866432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Exploratory Checks Using ROC AUC via prediction model XGBoost \n",
    "def roc_aud_conditional_independence_test(X, y, additional_var=None):\n",
    "    \"\"\"Test conditional independence using XGBoost and ROC AUC.\"\"\"\n",
    "    model = XGBClassifier(eval_metric=\"logloss\")\n",
    "    model.fit(X, y)\n",
    "    baseline_roc_auc = roc_auc_score(y, model.predict_proba(X)[:, 1])\n",
    "\n",
    "    if additional_var is not None:\n",
    "        X_with_additional = np.column_stack((X, additional_var))\n",
    "        model_with_additional = XGBClassifier(eval_metric=\"logloss\")\n",
    "        model_with_additional.fit(X_with_additional, y)\n",
    "        additional_roc_auc = roc_auc_score(y, model_with_additional.predict_proba(X_with_additional)[:, 1])\n",
    "        return baseline_roc_auc, additional_roc_auc\n",
    "    else:\n",
    "        return baseline_roc_auc\n",
    "\n"
   ],
   "id": "cb43aba7094e223c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:49:41.536361Z",
     "start_time": "2024-11-27T21:49:34.025829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test 1: Y^{\\bar{1}} тлл A0 | L0, U0\n",
    "X = data[['L_0', 'U_0']].values\n",
    "y = data['A_0'].values\n",
    "baseline_auc, auc_with_y = roc_aud_conditional_independence_test(X, y, data['Y_bar1'].values)\n",
    "print(f\"Test 1 (with U0): Baseline AUC={baseline_auc}, AUC with Y_bar1={auc_with_y}\")\n"
   ],
   "id": "1b03fe17963f4a60",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m X \u001B[38;5;241m=\u001B[39m data[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mL_0\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mU_0\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m      3\u001B[0m y \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mA_0\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m----> 4\u001B[0m baseline_auc, auc_with_y \u001B[38;5;241m=\u001B[39m \u001B[43mroc_aud_conditional_independence_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mY_bar1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest 1 (with U0): Baseline AUC=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbaseline_auc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, AUC with Y_bar1=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mauc_with_y\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[5], line 5\u001B[0m, in \u001B[0;36mroc_aud_conditional_independence_test\u001B[0;34m(X, y, additional_var)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Test conditional independence using XGBoost and ROC AUC.\"\"\"\u001B[39;00m\n\u001B[1;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m XGBClassifier(eval_metric\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogloss\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m baseline_roc_auc \u001B[38;5;241m=\u001B[39m roc_auc_score(y, model\u001B[38;5;241m.\u001B[39mpredict_proba(X)[:, \u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m additional_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ucsd1/lib/python3.9/site-packages/xgboost/core.py:726\u001B[0m, in \u001B[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[1;32m    725\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[0;32m--> 726\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ucsd1/lib/python3.9/site-packages/xgboost/sklearn.py:1531\u001B[0m, in \u001B[0;36mXGBClassifier.fit\u001B[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001B[0m\n\u001B[1;32m   1511\u001B[0m model, metric, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_configure_fit(xgb_model, params)\n\u001B[1;32m   1512\u001B[0m train_dmatrix, evals \u001B[38;5;241m=\u001B[39m _wrap_evaluation_matrices(\n\u001B[1;32m   1513\u001B[0m     missing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmissing,\n\u001B[1;32m   1514\u001B[0m     X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1528\u001B[0m     feature_types\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_types,\n\u001B[1;32m   1529\u001B[0m )\n\u001B[0;32m-> 1531\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_Booster \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1532\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1533\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dmatrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1534\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_num_boosting_rounds\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1535\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1536\u001B[0m \u001B[43m    \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1537\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevals_result\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevals_result\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1538\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1539\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcustom_metric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1540\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1541\u001B[0m \u001B[43m    \u001B[49m\u001B[43mxgb_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1542\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1545\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective):\n\u001B[1;32m   1546\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobjective \u001B[38;5;241m=\u001B[39m params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjective\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ucsd1/lib/python3.9/site-packages/xgboost/core.py:726\u001B[0m, in \u001B[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[1;32m    725\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[0;32m--> 726\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ucsd1/lib/python3.9/site-packages/xgboost/training.py:181\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cb_container\u001B[38;5;241m.\u001B[39mbefore_iteration(bst, i, dtrain, evals):\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m--> 181\u001B[0m \u001B[43mbst\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miteration\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfobj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cb_container\u001B[38;5;241m.\u001B[39mafter_iteration(bst, i, dtrain, evals):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ucsd1/lib/python3.9/site-packages/xgboost/core.py:2101\u001B[0m, in \u001B[0;36mBooster.update\u001B[0;34m(self, dtrain, iteration, fobj)\u001B[0m\n\u001B[1;32m   2097\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_dmatrix_features(dtrain)\n\u001B[1;32m   2099\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2100\u001B[0m     _check_call(\n\u001B[0;32m-> 2101\u001B[0m         \u001B[43m_LIB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mXGBoosterUpdateOneIter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2102\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\n\u001B[1;32m   2103\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2104\u001B[0m     )\n\u001B[1;32m   2105\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2106\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(dtrain, output_margin\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test 2: Y^{\\bar{1}} тлл A0 | L0\n",
    "X = data[['L_0']].values\n",
    "y = data['A_0'].values\n",
    "baseline_auc, auc_with_y = roc_aud_conditional_independence_test(X, y, data['Y_bar1'].values)\n",
    "print(f\"Test 2 (without U0): Baseline AUC={baseline_auc}, AUC with Y_bar1={auc_with_y}\")\n"
   ],
   "id": "653e0fc9ea2de068"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3 Check sequential conditional exchangeability is violated with only observed set of covaraites via ROC AUC \n",
   "id": "c82cb46cec3cc205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test 3: Y^{\\bar{1}} тлл A1 | A0, L0, U0, L1\n",
    "X = data[['A_0', 'L_0', 'U_0', 'L_1']].values\n",
    "y = data['A_1'].values\n",
    "baseline_auc, auc_with_y = roc_aud_conditional_independence_test(X, y, data['Y_bar1'].values)\n",
    "print(f\"Test 3 (with U0): Baseline AUC={baseline_auc}, AUC with Y_bar1={auc_with_y}\")\n"
   ],
   "id": "a252e3a8be9d88d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test 4: Y^{\\bar{1}} тлл A1 | A0, L0, L_1\n",
    "X = data[['A_0', 'L_0', 'L_1']].values\n",
    "y = data['A_1'].values\n",
    "baseline_auc, auc_with_y = roc_aud_conditional_independence_test(X, y, data['Y_bar1'].values)\n",
    "print(f\"Test 4 (without U0): Baseline AUC={baseline_auc}, AUC with Y_bar1={auc_with_y}\")\n"
   ],
   "id": "14413ad492a277a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Compute population quantities\n",
   "id": "772c931b6b5a3010"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:50:11.466294Z",
     "start_time": "2024-11-27T21:49:44.727567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Restrict to certain subset, follow the math expression\n",
    "\n",
    "# Split the data into training and prediction sets\n",
    "train_data = data[:10000000]   # First 1000 rows for training\n",
    "predict_data = data[10000000:].copy() # Last 1000 rows for prediction\n",
    "\n",
    "# # Step 1: estimate b1_true and attach to predict_data\n",
    "# train_subset_a0a1 = train_data[(train_data['A_0'] == 1) & (train_data['A_1'] == 1)]\n",
    "# X_train1 = train_subset_a0a1[['L_0', 'L_1', 'U_0']]\n",
    "# Y_train1 = train_subset_a0a1['Y']\n",
    "# model1 = LinearRegression()\n",
    "# model1.fit(X_train1, Y_train1)\n",
    "# X_predict1 = predict_data[['L_0', 'L_1', 'U_0']]\n",
    "# predict_data = predict_data.copy()  # Create an explicit copy to avoid SettingWithCopyWarning\n",
    "# predict_data.loc[:, 'b1_true'] = model1.predict(X_predict1)  # Use .loc for explicit assignment\n",
    "\n",
    "# Step 1: get b1_true from the data generating process directly\n",
    "predict_data['b1_true'] = (params[\"Y_L0_coeff\"] * predict_data['L_0'] +\n",
    "                           params[\"Y_L1_coeff\"] * predict_data['L_1'] +\n",
    "                           params[\"Y_A0_coeff\"] * 1 +  # Set A0=1\n",
    "                           params[\"Y_A1_coeff\"] * 1 +  # Set A1=1\n",
    "                           params[\"Y_U0_coeff\"] * predict_data['U_0'])\n",
    "\n",
    "# Step 2: estimate b1_short and attach to predict_data\n",
    "train_subset_a0a1 = train_data[(train_data['A_0'] == 1) & (train_data['A_1'] == 1)]\n",
    "X_train2 = train_subset_a0a1[['L_0', 'L_1']]\n",
    "Y_train2 = train_subset_a0a1['Y']\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X_train2, Y_train2)\n",
    "X_predict2 = predict_data[['L_0', 'L_1']]\n",
    "# No additional .copy() needed here as we already created one in Step 1\n",
    "predict_data.loc[:, 'b1_short'] = model2.predict(X_predict2)  # Use .loc for consistency\n",
    "\n",
    "# # Step 5: estimate pi1_true\n",
    "# train_subset_a0 = train_data[train_data['A_0'] == 1]\n",
    "# X_train5 = train_subset_a0[['L_0', 'L_1', 'U_0']]\n",
    "# Y_train5 = train_subset_a0['A_1']\n",
    "# model5 = LogisticRegression()\n",
    "# model5.fit(X_train5, Y_train5)\n",
    "# X_predict5 = predict_data[['L_0', 'L_1', 'U_0']]\n",
    "# predict_data.loc[:, 'pi1_true'] = model5.predict_proba(X_predict5)[:, 1]\n",
    "\n",
    "# Step 5: get pi1_true from the data generating process directly\n",
    "predict_data.loc[:, 'pi1_true'] = 1 / (1 + np.exp(-(params[\"A1_L1_coeff\"] * predict_data['L_1'] +\n",
    "                                            params[\"A1_A0_coeff\"] * 1 +  # Set A0 = 1\n",
    "                                            params[\"A1_L0_coeff\"] * predict_data['L_0'] +\n",
    "                                            params[\"A1_U0_coeff\"] * predict_data['U_0'])))\n",
    "\n",
    "# Step 6: estimate pi1_short\n",
    "train_subset_a0 = train_data[train_data['A_0'] == 1]\n",
    "X_train6 = train_subset_a0[['L_0', 'L_1']]\n",
    "Y_train6 = train_subset_a0['A_1']\n",
    "# model6 = LogisticRegression()\n",
    "# model6 = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=43)  # Nonparametric model\n",
    "model6 = XGBClassifier(\n",
    "    n_estimators=100,     # Number of trees\n",
    "    max_depth=5,          # Maximum depth of a tree\n",
    "    learning_rate=0.1,    # Learning rate (eta)\n",
    "    subsample=0.8,        # Subsample ratio of training data\n",
    "    colsample_bytree=0.8, # Subsample ratio of columns when constructing each tree\n",
    "    random_state=43       # Random seed for reproducibility\n",
    ")\n",
    "model6.fit(X_train6, Y_train6)\n",
    "X_predict6 = predict_data[['L_0', 'L_1']]\n",
    "predict_data.loc[:, 'pi1_short'] = model6.predict_proba(X_predict6)[:, 1]\n",
    "\n",
    "# # Step 7: estimate pi0_true\n",
    "# X_train7 = train_data[['L_0', 'U_0']]\n",
    "# Y_train7 = train_data['A_0']\n",
    "# model7 = LogisticRegression()\n",
    "# model7.fit(X_train7, Y_train7)\n",
    "# X_predict7 = predict_data[['L_0', 'U_0']]\n",
    "# predict_data.loc[:, 'pi0_true'] = model7.predict_proba(X_predict7)[:, 1]\n",
    "\n",
    "# Step 7: get pi0_true from the data generating process directly\n",
    "predict_data.loc[:, 'pi0_true'] = 1 / (1 + np.exp(-(params[\"A0_L0_coeff\"] * predict_data['L_0'] +\n",
    "                                            params[\"A0_U0_coeff\"] * predict_data['U_0'])))\n",
    "\n",
    "# Step 8: estimate pi0_short\n",
    "X_train8 = train_data[['L_0']]\n",
    "Y_train8 = train_data['A_0']\n",
    "# model8 = LogisticRegression()\n",
    "# model8 = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=43)  # Nonparametric model\n",
    "model8 = XGBClassifier(\n",
    "    n_estimators=100,     # Number of trees\n",
    "    max_depth=5,          # Maximum depth of a tree\n",
    "    learning_rate=0.1,    # Learning rate (eta)\n",
    "    subsample=0.8,        # Subsample ratio of training data\n",
    "    colsample_bytree=0.8, # Subsample ratio of columns when constructing each tree\n",
    "    random_state=43       # Random seed for reproducibility\n",
    ")\n",
    "model8.fit(X_train8, Y_train8)\n",
    "X_predict8 = predict_data[['L_0']]\n",
    "predict_data.loc[:, 'pi0_short'] = model8.predict_proba(X_predict8)[:, 1]\n",
    "\n",
    "# Step 3: estimate b0_true\n",
    "predict_train, predict_test = train_test_split(predict_data, test_size=0.5, random_state=42)\n",
    "predict_test = predict_test.copy()\n",
    "predict_train_subset = predict_train[predict_train['A_0'] == 1].copy()\n",
    "X_train3 = predict_train_subset[['L_0', 'U_0']]\n",
    "Y_train3 = predict_train_subset['b1_true']\n",
    "model3 = LinearRegression()\n",
    "model3.fit(X_train3, Y_train3)\n",
    "X_test3 = predict_test[['L_0', 'U_0']]\n",
    "predict_test.loc[:, 'b0_true'] = model3.predict(X_test3)  \n",
    "\n",
    "# Step 10: estimate ite_b1b0_short_true\n",
    "X_train10 = predict_train_subset[['L_0', 'U_0']]\n",
    "Y_train10 = predict_train_subset['b1_short']\n",
    "model10 = LinearRegression()\n",
    "model10.fit(X_train10, Y_train10)\n",
    "X_test10 = predict_test[['L_0', 'U_0']]\n",
    "predict_test.loc[:, 'ite_b1b0_short_true'] = model10.predict(X_test10)  \n",
    "\n",
    "# Step 4: estimate b0_short\n",
    "X_train4 = predict_train_subset[['L_0']]\n",
    "Y_train4 = predict_train_subset['b1_short']\n",
    "model4 = LinearRegression()\n",
    "model4.fit(X_train4, Y_train4)\n",
    "X_test4 = predict_test[['L_0']]\n",
    "predict_test.loc[:, 'b0_short'] = model4.predict(X_test4)"
   ],
   "id": "d93300bfbea9a6de",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:50:13.415014Z",
     "start_time": "2024-11-27T21:50:13.210271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute important quantities \n",
    "a0pi0_short = predict_test['A_0'] / predict_test['pi0_short']\n",
    "a1pi1_short = predict_test['A_1'] / predict_test['pi1_short']\n",
    "a0pi0_true = predict_test['A_0'] / predict_test['pi0_true']\n",
    "a1pi1_true = predict_test['A_1'] / predict_test['pi1_true']\n",
    "\n",
    "diff_a0pi0 = a0pi0_true - a0pi0_short\n",
    "diff_a1pi1 = a1pi1_true - a1pi1_short\n",
    "\n",
    "diff_b0 = predict_test['b0_short'] - predict_test['b0_true']\n",
    "diff_b1 = predict_test['b1_short'] - predict_test['b1_true']\n"
   ],
   "id": "4de449d26a358cc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Compute the population quantity of mean potential outcome under treatment path $\\bar{A} = \\bar{1}$ (1)\n",
    "\n"
   ],
   "id": "613da43feb06d44d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:50:16.421970Z",
     "start_time": "2024-11-27T21:50:15.667212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mean_Y_bar1 = np.mean(Y_bar1)\n",
    "display(Math(r\"E[Y^{{\\bar{{1}}}}] = {:.4f}\".format(mean_Y_bar1)))"
   ],
   "id": "bbfb52425a2a9e98",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Math object>"
      ],
      "text/latex": "$\\displaystyle E[Y^{\\bar{1}}] = 1.5197$"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Compute the population AIPW $\\psi$ based on (8)\n",
   "id": "84054c5225bd9719"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:50:17.985222Z",
     "start_time": "2024-11-27T21:50:17.853784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate psi_true\n",
    "psi_true = np.mean(predict_test['b0_true'] - a0pi0_true * predict_test['b0_true']\n",
    "            - a0pi0_true * a1pi1_true * predict_test['b1_true']\n",
    "            + a0pi0_true * predict_test['b1_true']\n",
    "            + a0pi0_true * a1pi1_true * predict_test['Y'])\n",
    "\n",
    "display(Math(r\"\\psi = {:.4f}\".format(psi_true)))\n"
   ],
   "id": "e6bf3e83b6511452",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Math object>"
      ],
      "text/latex": "$\\displaystyle \\psi = 1.5320$"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3 Compute the incorrect population AIPW $\\psi^*$ based on (11)\n",
   "id": "c5979786d51d582"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:50:19.880743Z",
     "start_time": "2024-11-27T21:50:19.741160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate psi_short\n",
    "psi_short = np.mean(predict_test['b0_short'] - a0pi0_short * predict_test['b0_short']\n",
    "            - a0pi0_short * a1pi1_short * predict_test['b1_short']\n",
    "            + a0pi0_short * predict_test['b1_short']\n",
    "            + a0pi0_short * a1pi1_short * predict_test['Y'])\n",
    "\n",
    "display(Math(r\"\\psi^* = {:.4f}\".format(psi_short)))\n"
   ],
   "id": "babe4841da895629",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Math object>"
      ],
      "text/latex": "$\\displaystyle \\psi^* = 2.9970$"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.4 Compute their difference $\\psi^*-\\psi$ as listed (12)\n",
   "id": "2becbc1d04381934"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:50:21.705676Z",
     "start_time": "2024-11-27T21:50:21.688323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "diff_psi = psi_short - psi_true\n",
    "display(Math(r\"\\psi^* - \\psi = {:.4f}\".format(diff_psi)))\n"
   ],
   "id": "a3ea77af331a5d07",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Math object>"
      ],
      "text/latex": "$\\displaystyle \\psi^* - \\psi = 1.4650$"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.5 Compute the basic ovb based on equation (13)\n",
   "id": "c584bd8278635cc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:50:23.563670Z",
     "start_time": "2024-11-27T21:50:23.485782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ovb_basic = np.mean(diff_b0 * diff_a0pi0 + a0pi0_short * diff_b1 * diff_a1pi1)\n",
    "print(f\"Basic OVB Formula = {ovb_basic:.4f}\")"
   ],
   "id": "ec8c9a0bf94a9d18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic OVB Formula = 1.4809\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.6 Compute updated ovb based on equation (26)",
   "id": "e9c5678aaeeba68b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:50:29.563700Z",
     "start_time": "2024-11-27T21:50:29.442845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "T1 = np.mean((predict_test['b0_short']- predict_test['ite_b1b0_short_true']) * diff_a0pi0)\n",
    "T2 = np.mean((predict_test['ite_b1b0_short_true'] - predict_test['b0_true']) * diff_a0pi0)\n",
    "T3 = np.mean(a0pi0_short * diff_b1 * diff_a1pi1)\n",
    "\n",
    "sum_T1_T2_T3 = T1 + T2 + T3\n",
    "\n",
    "print(f\"T1 + T2 + T3 = {sum_T1_T2_T3:.4f}\")"
   ],
   "id": "1c4ee9a9257e276f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 + T2 + T3 = 1.4809\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.7 Compute the ovb bound based on (14) together with (18) (19) (20)",
   "id": "d1c45eef99d7a2df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:50:32.382212Z",
     "start_time": "2024-11-27T21:50:32.242750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "T11_square = np.mean((predict_test['b0_short']- predict_test['ite_b1b0_short_true']) ** 2)\n",
    "T12_square = np.mean(diff_a0pi0 ** 2)\n",
    "T21_square = np.mean((predict_test['ite_b1b0_short_true'] - predict_test['b0_true']) ** 2)\n",
    "T21_square_prime = np.mean(diff_b1 ** 2)\n",
    "T22_square = np.mean(diff_a0pi0 ** 2)\n",
    "T31_square = np.mean(a0pi0_short * (diff_b1 ** 2))\n",
    "T32_square = np.mean(a0pi0_short * (diff_a1pi1 ** 2))\n",
    "\n",
    "T1_square_ub = T11_square * T12_square\n",
    "T2_square_ub = T21_square * T22_square\n",
    "T2_square_prime_ub = T21_square_prime * T22_square\n",
    "T3_square_ub = T31_square * T32_square\n",
    "print(T1_square_ub, T2_square_prime_ub, T3_square_ub)\n",
    "\n",
    "T1_ub = np.sqrt(T1_square_ub)\n",
    "T2_ub = np.sqrt(T2_square_ub)\n",
    "T2_prime_ub = np.sqrt(T2_square_prime_ub)\n",
    "T3_ub = np.sqrt(T3_square_ub)\n",
    "\n",
    "ovb_prime_ub = T1_ub + T2_prime_ub + T3_ub\n",
    "display(Math(r\"|\\psi^* - \\psi| \\le |T_1| + |T_2| + |T_3| \\le {:.4f}\".format(ovb_prime_ub)))\n"
   ],
   "id": "6f4d19e0a0bf3eb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.336939953447173 10.636513203822107 0.5354968204113605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Math object>"
      ],
      "text/latex": "$\\displaystyle |\\psi^* - \\psi| \\le |T_1| + |T_2| + |T_3| \\le 6.3033$"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.8 Compute the ovb bound based on (14), (18) (19) (20), and corollary 1.",
   "id": "fd8b1c60b1db14b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:50:34.689485Z",
     "start_time": "2024-11-27T21:50:34.236779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "C1b1_square = (np.mean((predict_test['b0_short'] - predict_test['ite_b1b0_short_true']) ** 2) / \n",
    "               np.mean((predict_test['b1_short'] - predict_test['ite_b1b0_short_true']) ** 2))\n",
    "S1_square = np.mean((predict_test['b1_short'] - predict_test['ite_b1b0_short_true']) ** 2) * np.mean(a0pi0_short ** 2)\n",
    "Ca0_square = np.mean(diff_a0pi0 ** 2) / np.mean(a0pi0_short ** 2)\n",
    "\n",
    "C1b1_square_S1_square_Ca0_square = C1b1_square * S1_square * Ca0_square\n",
    "\n",
    "Cy_square = np.mean(diff_b1 ** 2) / np.mean((predict_test['Y'] - predict_test['b1_short']) ** 2)\n",
    "S2_square = np.mean((predict_test['Y'] - predict_test['b1_short']) ** 2) * np.mean(a0pi0_short ** 2)\n",
    "\n",
    "Cy_square_S2_square_Ca0_square =  Cy_square * S2_square * Ca0_square\n",
    "\n",
    "C_tilde_y_square = np.mean(a0pi0_short * (diff_b1 ** 2)) / np.mean(a0pi0_short * ((predict_test['Y'] - predict_test['b1_short']) ** 2)) \n",
    "S3_square = np.mean(a0pi0_short * ((predict_test['Y'] - predict_test['b1_short']) ** 2)) * np.mean(a0pi0_short * (a1pi1_short ** 2))\n",
    "C_tilde_a1_square = np.mean(a0pi0_short * (diff_a1pi1 ** 2)) / np.mean(a0pi0_short * (a1pi1_short ** 2))\n",
    "\n",
    "C_tilde_y_square_S3_square_C_tilde_a1_square = C_tilde_y_square * S3_square * C_tilde_a1_square\n",
    "\n",
    "display(Math(r\"C_{{b_1^*}}^2 S_1^2 C_{{A_0}}^2 = {:.4f}\".format(C1b1_square_S1_square_Ca0_square)))\n",
    "display(Math(r\"C_{{Y}}^2 S_2^2 C_{{A_0}}^2 = {:.4f}\".format(Cy_square_S2_square_Ca0_square)))\n",
    "display(Math(r\"C_{{\\tilde{{Y}}}}^2 S_3^2 C_{{\\tilde{{A}}_1}}^2 = {:.4f}\".format(C_tilde_y_square_S3_square_C_tilde_a1_square)))\n",
    "\n"
   ],
   "id": "d03f9db3637d387d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Math object>"
      ],
      "text/latex": "$\\displaystyle C_{b_1^*}^2 S_1^2 C_{A_0}^2 = 5.3369$"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Math object>"
      ],
      "text/latex": "$\\displaystyle C_{Y}^2 S_2^2 C_{A_0}^2 = 10.6365$"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Math object>"
      ],
      "text/latex": "$\\displaystyle C_{\\tilde{Y}}^2 S_3^2 C_{\\tilde{A}_1}^2 = 0.5355$"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.9 Summary table\n",
   "id": "ee182fb8237ce90d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T21:50:36.628067Z",
     "start_time": "2024-11-27T21:50:36.618846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Define the quantities to include in the summary table\n",
    "summary_data = {\n",
    "    \"Row Name\": [\n",
    "        r\"$\\mathbb{E}[Y^{\\bar{1}}]$\",\n",
    "        r\"$\\psi$\",\n",
    "        r\"$\\psi^*$\",\n",
    "        r\"$\\psi^* - \\psi$\",\n",
    "        r\"Basic OVB\",\n",
    "        r\"$T_1 + T_2 + T_3$\",\n",
    "        r\"$|\\psi^* - \\psi| \\leq |T_1| + |T_2| + |T_3| \\leq (C_{{b_1^*}}^2 S_1^2 C_{{A_0}}^2)^{1/2} + (C_{{Y}}^2 S_2^2 C_{{A_0}}^2)^{1/2} + (C_{{\\tilde{{Y}}}}^2 S_3^2 C_{{\\tilde{{A}}_1}}^2)^{1/2}$\"\n",
    "    ],\n",
    "    \"Value\": [\n",
    "        mean_Y_bar1,  \n",
    "        psi_true,  \n",
    "        psi_short,  \n",
    "        diff_psi,  \n",
    "        ovb_basic,  \n",
    "        sum_T1_T2_T3,  \n",
    "        ovb_prime_ub  \n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the table header with spacing\n",
    "markdown_table = \"| Row Name                              | Value     |\\n\"\n",
    "markdown_table += \"|---------------------------------------|-----------|\\n\"\n",
    "\n",
    "# Add each row to the table with better spacing\n",
    "for name, value in zip(summary_data[\"Row Name\"], summary_data[\"Value\"]):\n",
    "    markdown_table += f\"| {name}                                | {value:.6f} |\\n\"\n",
    "\n",
    "# Display the formatted table\n",
    "display(Markdown(markdown_table))\n"
   ],
   "id": "efbc67de9c428785",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "| Row Name                              | Value     |\n|---------------------------------------|-----------|\n| $\\mathbb{E}[Y^{\\bar{1}}]$                                | 1.519690 |\n| $\\psi$                                | 1.532026 |\n| $\\psi^*$                                | 2.997007 |\n| $\\psi^* - \\psi$                                | 1.464980 |\n| Basic OVB                                | 1.480941 |\n| $T_1 + T_2 + T_3$                                | 1.480941 |\n| $|\\psi^* - \\psi| \\leq |T_1| + |T_2| + |T_3| \\leq (C_{{b_1^*}}^2 S_1^2 C_{{A_0}}^2)^{1/2} + (C_{{Y}}^2 S_2^2 C_{{A_0}}^2)^{1/2} + (C_{{\\tilde{{Y}}}}^2 S_3^2 C_{{\\tilde{{A}}_1}}^2)^{1/2}$                                | 6.303325 |\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
